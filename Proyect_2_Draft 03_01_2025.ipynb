{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergio-qp/AP_2_SQ_AG/blob/master/Proyect_2_Draft%2003_01_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "tX9VgN1lJq2V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from google.colab import files\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "qgmI1o4vAX-O",
        "outputId": "82291ae2-0cef-433e-e0df-bf0a46047f67"
      },
      "outputs": [],
      "source": [
        "\n",
        "# uploaded=files.upload()\n",
        "# file_name = list(uploaded.keys())[0]\n",
        "file_name = \"attrition_availabledata_02.csv\"\n",
        "data = pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z7tc1Hjl-OwF",
        "outputId": "608670ca-99e7-4312-90ce-ca049fad0d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         hrs  absences  JobInvolvement  PerformanceRating  \\\n",
            "0  10.060048       6.0             3.0                4.0   \n",
            "1   9.437671       2.0             2.0                3.0   \n",
            "2   7.900932      20.0             3.0                4.0   \n",
            "3   7.193853      19.0             4.0                3.0   \n",
            "4   6.979201       8.0             3.0                3.0   \n",
            "\n",
            "   EnvironmentSatisfaction  JobSatisfaction  WorkLifeBalance   Age  \\\n",
            "0                      2.0              4.0              1.0  31.0   \n",
            "1                      3.0              4.0              3.0  33.0   \n",
            "2                      3.0              4.0              3.0  35.0   \n",
            "3                      4.0              2.0              3.0  28.0   \n",
            "4                      2.0              4.0              2.0  31.0   \n",
            "\n",
            "      BusinessTravel              Department  ...  Over18  PercentSalaryHike  \\\n",
            "0  Travel_Frequently  Research & Development  ...       Y               23.0   \n",
            "1         Non-Travel  Research & Development  ...       Y               13.0   \n",
            "2      Travel_Rarely  Research & Development  ...       Y               22.0   \n",
            "3      Travel_Rarely  Research & Development  ...       Y               15.0   \n",
            "4      Travel_Rarely  Research & Development  ...       Y               12.0   \n",
            "\n",
            "  StandardHours  StockOptionLevel  TotalWorkingYears TrainingTimesLastYear  \\\n",
            "0           8.0               1.0                7.0                   5.0   \n",
            "1           8.0               0.0                7.0                   6.0   \n",
            "2           8.0               1.0               10.0                   4.0   \n",
            "3           8.0               0.0                1.0                   1.0   \n",
            "4           8.0               1.0               10.0                   2.0   \n",
            "\n",
            "   YearsAtCompany YearsSinceLastPromotion YearsWithCurrManager  Attrition  \n",
            "0             2.0                     2.0                  2.0        Yes  \n",
            "1             6.0                     1.0                  2.0         No  \n",
            "2            10.0                     7.0                  7.0        Yes  \n",
            "3             1.0                     0.0                  0.0         No  \n",
            "4             8.0                     7.0                  7.0         No  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_dnEXuqC5jN",
        "outputId": "4f4024be-9f9c-4977-add5-6f9a00784299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constant Columns Removed: ['EmployeeCount', 'Over18', 'StandardHours']\n",
            "Number of Rows: 2940\n",
            "Number of Columns: 31\n",
            "Column Types: {'float64': 23, 'object': 8}\n",
            "Categorical Variables: ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'Attrition']\n",
            "Numerical Variables: ['hrs', 'absences', 'JobInvolvement', 'PerformanceRating', 'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age', 'DistanceFromHome', 'Education', 'EmployeeCount', 'EmployeeID', 'JobLevel', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
            "Categorical Variables Table:          Variable  Cardinality\n",
            "0  BusinessTravel            3\n",
            "1      Department            3\n",
            "2  EducationField            6\n",
            "3          Gender            2\n",
            "4         JobRole            9\n",
            "5   MaritalStatus            3\n",
            "6          Over18            1\n",
            "7       Attrition            2\n",
            "Missing Values: There are no missing values\n",
            "Removed Constant Columns: ['EmployeeCount', 'Over18', 'StandardHours']\n",
            "Possible ID Columns: ['EmployeeID']\n",
            "Problem Type: classification\n",
            "Class Distribution (if classification): Attrition\n",
            "No     0.838776\n",
            "Yes    0.161224\n",
            "Name: count, dtype: float64\n",
            "Is Imbalanced: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# General Information\n",
        "num_rows = len(data)  # Number of rows\n",
        "num_columns = len(data.columns)  # Number of columns\n",
        "\n",
        "# Column Types\n",
        "column_types = data.dtypes  # Data types of the columns\n",
        "column_info = {}\n",
        "for dtype in column_types.unique():\n",
        "    column_info[str(dtype)] = sum(column_types == dtype)\n",
        "\n",
        "# Identify categorical and numerical variables\n",
        "categorical_vars = [col for col in data.columns if data[col].dtype == 'object']\n",
        "numerical_vars = [col for col in data.columns if data[col].dtype in ['int64', 'float64']]\n",
        "\n",
        "# Cardinality of categorical variables\n",
        "categorical_cardinality = {}\n",
        "for col in categorical_vars:\n",
        "    categorical_cardinality[col] = data[col].nunique()\n",
        "\n",
        "# Missing Values\n",
        "if data.isnull().sum().sum() > 0:\n",
        "    missing_values = \"There are missing values\"\n",
        "else:\n",
        "    missing_values = \"There are no missing values\"\n",
        "\n",
        "# Constant Columns\n",
        "constant_columns = [col for col in data.columns if data[col].nunique() == 1]\n",
        "print(f\"Constant Columns Removed: {constant_columns}\")\n",
        "\n",
        "# ID Columns\n",
        "possible_id_columns = []\n",
        "for col in data.columns:\n",
        "    if data[col].nunique() == num_rows:\n",
        "        possible_id_columns.append(col)\n",
        "\n",
        "# Problem Type (Regression or Classification)\n",
        "target_variable = data.columns[-1]  # Last column as target\n",
        "if data[target_variable].nunique() <= 10:\n",
        "    target_type = 'classification'\n",
        "else:\n",
        "    target_type = 'regression'\n",
        "\n",
        "# Class Distribution (if classification)\n",
        "if target_type == 'classification':\n",
        "    class_counts = data[target_variable].value_counts()  # Count each class\n",
        "    total_counts = len(data[target_variable])  # Total rows\n",
        "    class_distribution = class_counts / total_counts  # Proportion of each class\n",
        "\n",
        "    # Check if the dataset is imbalanced\n",
        "    is_imbalanced = class_distribution.max() > 0.6  # Use .max() with parentheses\n",
        "else:\n",
        "    class_distribution = None\n",
        "    is_imbalanced = None\n",
        "\n",
        "# EDA Summary\n",
        "eda_summary = {\n",
        "    \"Number of Rows\": num_rows,\n",
        "    \"Number of Columns\": num_columns,\n",
        "    \"Column Types\": column_info,\n",
        "    \"Categorical Variables\": categorical_vars,\n",
        "    \"Numerical Variables\": numerical_vars,\n",
        "    \"Categorical Variables Table\": pd.DataFrame({\n",
        "        \"Variable\": categorical_vars,\n",
        "        \"Cardinality\": [categorical_cardinality[col] for col in categorical_vars]\n",
        "    }),\n",
        "    \"Missing Values\": missing_values,\n",
        "    \"Removed Constant Columns\": constant_columns,\n",
        "    \"Possible ID Columns\": possible_id_columns,\n",
        "    \"Problem Type\": target_type,\n",
        "    \"Class Distribution (if classification)\": class_distribution,\n",
        "    \"Is Imbalanced\": is_imbalanced,\n",
        "}\n",
        "\n",
        "for key, value in eda_summary.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PdAQKikygqn"
      },
      "source": [
        "The dataset contains 2940 rows and 31 columns. Of these 31 variables, 23 are numerical variables, and 8 are categorical variables. These categorical variables include: BusinessTravel, Department, or Gender. They have different values; for example, Gender has two unique values: \"Female\" and \"Male.\" On the other hand, there are other categorical variables, such as JobRole, which have 9 unique values, indicating different roles like Sales Executive or Manager.\n",
        "\n",
        "Before performing any statistical analysis, it is crucial to check if the dataset contains missing values, as they can cause errors. As observed, the dataset does not have any missing values. Similarly, it is essential to verify if there are variables that have the same value across all observations, as these variables could be removed. In this dataset, Over18, EmployeeCount, and StandardHours are constant variables, which do not provide any useful information and should be eliminated.\n",
        "\n",
        "To determine whether we have a classification or regression problem, it is necessary to identify our target variable. The target variable is Attrition, whose unique values are \"Yes\" and \"No.\" Therefore, as it is a categorical variable, this is a classification problem.\n",
        "\n",
        "The target variable presents significant imbalance, with 83% of the observations corresponding to employees who do not leave (\"No\") and 16% belonging to the \"Yes\" category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofduxB81GLVa",
        "outputId": "230e69a8-46c7-4535-f84d-c0896503c81f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attrition\n",
            "0    2466\n",
            "1     474\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sergi\\AppData\\Local\\Temp\\ipykernel_25424\\1982002934.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y = data['Attrition'].replace({'No': 0, 'Yes': 1})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop(columns=['Attrition'])  # Drop target variable\n",
        "y = data['Attrition'].replace({'No': 0, 'Yes': 1})\n",
        "print(y.value_counts())\n",
        "\n",
        "# Division de los datos\n",
        "# Split the dataset into training and testing sets (80/20 split)\n",
        "seed = 100533387  # Set student id seed for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g3aXDZyQlW-"
      },
      "source": [
        "We split our data 80/20 into train and test sets so we can use them for the Holdout method when we want to estimate future performance later.\n",
        "\n",
        "For inner evaluation, we used 3-fold StratifiedKFold since the target variable is imbalanced. This way, we maintain the proportion of classes in our samples to ensure that one sample does not contain only one class of the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IN048_cfW3w",
        "outputId": "336876ba-e686-44e0-b8e1-ac3307b0406f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Identify categorical and numerical columns\n",
        "# This separates the features into categorical and numerical types for preprocessing.\n",
        "categorical_vars = X.select_dtypes(include=['object']).columns.tolist()  # List of categorical columns\n",
        "numerical_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()  # List of numerical columns\n",
        "\n",
        "# Step 2: Create a preprocessor\n",
        "# A ColumnTransformer is used to apply different transformations to categorical and numerical columns.\n",
        "# - Categorical: OneHotEncoder to convert categories into binary columns.\n",
        "# - Numerical: 'passthrough' means numerical columns are not transformed.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_vars),  # Transform categorical variables\n",
        "        ('num', 'passthrough', numerical_vars)  # Keep numerical variables as is\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
        "inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed) # set up our inner 3fold cv evaluation method\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 1: Dummy Classifier (Baseline)\n",
        "# A simple classifier that always predicts the most frequent class. This is used as a baseline for comparison.\n",
        "start_dummy = time.time()  # Start timing\n",
        "dummy_clf = DummyClassifier(strategy=\"most_frequent\", random_state=seed)  # Most frequent strategy\n",
        "dummy_clf.fit(X_train, y_train)  # Train on the training data\n",
        "dummy_cv_score = -cross_val_score(dummy_clf, X_train, y_train,cv=inner, scoring='neg_root_mean_squared_error').mean() # Get our 3fold CV score\n",
        "dummy_time = time.time() - start_dummy  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 2: Decision Tree with Pipeline\n",
        "# Combines the preprocessor and a Decision Tree model into a single pipeline for clean and efficient workflow.\n",
        "start_tree = time.time()  # Start timing\n",
        "tree_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing to the data\n",
        "    ('classifier', DecisionTreeClassifier(random_state=seed))  # Train a Decision Tree classifier\n",
        "])\n",
        "tree_pipeline.fit(X_train, y_train)  # Train the pipeline\n",
        "tree_cv_score = -cross_val_score(tree_pipeline, X_train, y_train,cv=inner, scoring='neg_root_mean_squared_error').mean() # Get our 3fold CV score\n",
        "tree_time = time.time() - start_tree  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 3: KNN with StandardScaler\n",
        "# Combines preprocessing, scaling (StandardScaler), and KNN model into a single pipeline.\n",
        "start_knn_std = time.time()  # Start timing\n",
        "knn_pipeline_std = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing\n",
        "    ('scaler', StandardScaler()),  # Scale features to have mean=0 and std=1\n",
        "    ('classifier', KNeighborsClassifier())  # Train a KNN classifier\n",
        "])\n",
        "knn_pipeline_std.fit(X_train, y_train)  # Train the pipeline\n",
        "knn_std_cv_score = -cross_val_score(knn_pipeline_std, X_train, y_train,cv=inner, scoring='neg_root_mean_squared_error').mean() # Get our 3fold CV score\n",
        "knn_std_time = time.time() - start_knn_std  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 4: KNN with MinMaxScaler\n",
        "# Similar to the StandardScaler pipeline, but uses MinMaxScaler for scaling.\n",
        "start_knn_minmax = time.time()  # Start timing\n",
        "knn_pipeline_minmax = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing\n",
        "    ('scaler', MinMaxScaler()),  # Scale features to range [0, 1]\n",
        "    ('classifier', KNeighborsClassifier())  # Train a KNN classifier\n",
        "])\n",
        "knn_pipeline_minmax.fit(X_train, y_train)  # Train the pipeline\n",
        "knn_minmax_cv_score = -cross_val_score(knn_pipeline_minmax, X_train, y_train,cv=inner, scoring='neg_root_mean_squared_error').mean() # Get our 3fold CV score\n",
        "knn_minmax_time = time.time() - start_knn_minmax  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results:\n",
            "Dummy Classifier - RMSE: 0.40, Time: 0.01s\n",
            "Decision Tree - RMSE: 0.35, Time: 0.20s\n",
            "KNN (StandardScaler) - RMSE: 0.41, Time: 0.17s\n",
            "KNN (MinMaxScaler) - RMSE: 0.42, Time: 0.17s\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Display results\n",
        "# Print the performance metrics (Accuracy and Balanced Accuracy) for all models.\n",
        "# Additionally, display the time taken for each model.\n",
        "print(\"Results:\")\n",
        "print(f\"Dummy Classifier - RMSE: {dummy_cv_score:.2f}, Time: {dummy_time:.2f}s\")\n",
        "print(f\"Decision Tree - RMSE: {tree_cv_score:.2f}, Time: {tree_time:.2f}s\")\n",
        "print(f\"KNN (StandardScaler) - RMSE: {knn_std_cv_score:.2f}, Time: {knn_std_time:.2f}s\")\n",
        "print(f\"KNN (MinMaxScaler) - RMSE: {knn_minmax_cv_score:.2f}, Time: {knn_minmax_time:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21FJ8sL5heoy"
      },
      "source": [
        "For this section, the following procedures were carried out:\n",
        "\n",
        "Identification of categorical and numerical columns: As performed in the EDA section, categorical and numerical variables were identified.\n",
        "Transformation of categorical variables: Categorical variables were converted into binary variables using OneHotEncoder.\n",
        "Dummy Classifier: This model predicts the most frequent class in the dataset.\n",
        "Decision Tree and KNN: A Decision Tree model was implemented, as well as KNN, which compares distances between data points. For KNN, the scales of variables can significantly impact the model's performance. Therefore, two types of scaling methods were compared: StandardScaler and MinMaxScaler.\n",
        "\n",
        "The effectiveness of the various classification methods was calculated and compared. We observed that the dummy classifier was by far the fastest, while all the other models took around the same time as each other to execute. Given that the KNN standard scaler was slightly better than the minmax one and executed slightly faster, we decided to use this one in the following steps of our project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HPO to get tuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Trees with Grid-Search\n",
        "\n",
        "# just a couple of arrays with different parameters to test, IMPORTANT: classifier__ needs to go in front to be used in pipeline\n",
        "param_grid_tree = {'classifier__max_depth': [10, 20, 30], 'classifier__min_samples_split': [2, 10, 20]} \n",
        "start = time.time()  # Start timing\n",
        "\n",
        "# we use our preexisting tree pipeline\n",
        "grid_search_tree = GridSearchCV(tree_pipeline,param_grid_tree, cv=inner, scoring='neg_root_mean_squared_error')\n",
        "grid_search_tree.fit(X_train, y_train)\n",
        "\n",
        "tg_best_inner = -grid_search_tree.best_score_\n",
        "tg_best_hp = grid_search_tree.best_params_\n",
        "\n",
        "tree_grid_time = time.time() - start  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Trees with Random-Search\n",
        "\n",
        "# param distribution this time\n",
        "param_dist_tree = {'classifier__max_depth':  np.arange(10, 31, 1), 'classifier__min_samples_split': np.arange(2, 21, 1)} \n",
        "start = time.time()  # Start timing\n",
        "\n",
        "# we use our preexisting tree pipeline\n",
        "random_search_tree = RandomizedSearchCV(tree_pipeline,param_dist_tree, cv=inner, scoring='neg_root_mean_squared_error', n_iter=10, random_state=seed)\n",
        "random_search_tree.fit(X_train, y_train)\n",
        "\n",
        "tr_best_inner = -random_search_tree.best_score_\n",
        "tr_best_hp = random_search_tree.best_params_\n",
        "tree_random_time = time.time() - start  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN with Grid-Search\n",
        "\n",
        "param_grid_knn = {'classifier__n_neighbors': [3,5,7],\n",
        "                  'classifier__weights': [\"uniform\",\"distance\"]} \n",
        "# we use our preexisting tree pipeline\n",
        "start = time.time()  # Start timing\n",
        "\n",
        "grid_search_knn = GridSearchCV(knn_pipeline_std,param_grid_knn, cv=inner, scoring='neg_root_mean_squared_error')\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "\n",
        "kg_best_inner = -grid_search_knn.best_score_\n",
        "kg_best_hp = grid_search_knn.best_params_\n",
        "knn_grid_time = time.time() - start  # End timing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN with Random-Search\n",
        "param_dist_knn = {'classifier__n_neighbors': np.arange(3, 9, 1),\n",
        "                  'classifier__weights': [\"uniform\",\"distance\"]} \n",
        "start = time.time()  # Start timing\n",
        "\n",
        "# we use our preexisting tree pipeline\n",
        "random_search_knn = RandomizedSearchCV(knn_pipeline_std,param_dist_knn, cv=inner, scoring='neg_root_mean_squared_error', n_iter=10, random_state=seed)\n",
        "random_search_knn.fit(X_train, y_train)\n",
        "\n",
        "kr_best_inner = -random_search_knn.best_score_\n",
        "kr_best_hp = random_search_knn.best_params_\n",
        "knn_random_time = time.time() - start  # End timing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results:\n",
            "Decision Trees (Base) - Inner RMSE: 0.35399082718999325, Best Hyperparameters: Default, Time: 0.20300507545471191s\n",
            "Decision Trees (Grid-Search) - Inner RMSE: 0.35025633206842227, Best Hyperparameters: {'classifier__max_depth': 10, 'classifier__min_samples_split': 2}, Time: 1.3984735012054443s\n",
            "Decision Trees (Random-Search) - Inner RMSE: 0.34382900580296116, Best Hyperparameters: {'classifier__min_samples_split': np.int64(3), 'classifier__max_depth': np.int64(19)}, Time: 1.38356351852417s\n",
            "KNN (Base) - Inner RMSE: 0.4056097551376951, Best Hyperparameters: Default, Time: 0.1670529842376709s\n",
            "KNN (Grid-Search) - Inner RMSE: 0.33047533978831606, Best Hyperparameters: {'classifier__n_neighbors': 3, 'classifier__weights': 'distance'}, Time: 0.9070005416870117s\n",
            "KNN (Random-Search) - Inner RMSE: 0.33047533978831606, Best Hyperparameters: {'classifier__weights': 'distance', 'classifier__n_neighbors': np.int64(3)}, Time: 1.5321123600006104s\n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "\n",
        "print(\"Results:\")\n",
        "print(f\"Decision Trees (Base) - Inner RMSE: {tree_cv_score}, Best Hyperparameters: Default, Time: {tree_time}s\")\n",
        "print(f\"Decision Trees (Grid-Search) - Inner RMSE: {tg_best_inner}, Best Hyperparameters: {tg_best_hp}, Time: {tree_grid_time}s\")\n",
        "print(f\"Decision Trees (Random-Search) - Inner RMSE: {tr_best_inner}, Best Hyperparameters: {tr_best_hp}, Time: {tree_random_time}s\")\n",
        "print(f\"KNN (Base) - Inner RMSE: {knn_std_cv_score}, Best Hyperparameters: Default, Time: {knn_std_time}s\")\n",
        "print(f\"KNN (Grid-Search) - Inner RMSE: {kg_best_inner}, Best Hyperparameters: {kg_best_hp}, Time: {knn_grid_time}s\")\n",
        "print(f\"KNN (Random-Search) - Inner RMSE: {kr_best_inner}, Best Hyperparameters: {kr_best_hp}, Time: {knn_random_time}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGCx49x_dRnZ"
      },
      "source": [
        "### Decision Trees\n",
        "We observed a slight improvement over the base parameters (`RMSE: 0.337`) using grid search (`RMSE: 0.331`), but a decrease in performance when using random search (`RMSE: 0.344`). Given the comparatively larger depth and min split in the random example, it appears to be a case of overfitting that led to a performance poorer than the base classifier. As such, within decision trees, the grid search tuned model with a max depth of 10 and min split of 2 performed the best.\n",
        "\n",
        "### KNN\n",
        "In the case of KNN, both models returned identical parameters (`RMSE: 0.321`) and therefore identical performance, although grid search was slightly faster. Both performed better than the base KNN classifier (`RMSE: 0.408`), which was the worst performing overall.\n",
        "\n",
        "### Summary\n",
        "Regarding execution speed, the tuning step greatly slowed the overall performance of all tuned models in comparison to their base versions. Grid search generally performed slightly faster than random search, which makes sense given that grid search depends on the amount of parameters input to be tested, which was generally around 3 per field, while random depended on the amount of iterations, which we kept at 10. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Comparison Results:\n",
            "                           Method  Inner RMSE  Time (s)\n",
            "0                Dummy Classifier    0.401421  0.014030\n",
            "1            Decision Tree (Base)    0.353991  0.203005\n",
            "2    Decision Trees (Grid-Search)    0.350256  1.398474\n",
            "3  Decision Trees (Random-Search)    0.343829  1.383564\n",
            "4                      KNN (Base)    0.405610  0.167053\n",
            "5               KNN (Grid-Search)    0.330475  0.907001\n",
            "6             KNN (Random-Search)    0.330475  1.532112\n"
          ]
        }
      ],
      "source": [
        "\n",
        "results = {\n",
        "    \"Method\": [\n",
        "        \"Dummy Classifier\",\n",
        "        \"Decision Tree (Base)\",\n",
        "        \"Decision Trees (Grid-Search)\",\n",
        "        \"Decision Trees (Random-Search)\",\n",
        "        \"KNN (Base)\",\n",
        "        \"KNN (Grid-Search)\",\n",
        "        \"KNN (Random-Search)\"\n",
        "    ],\n",
        "    \"Inner RMSE\": [\n",
        "        dummy_cv_score,\n",
        "        tree_cv_score,\n",
        "        tg_best_inner,\n",
        "        tr_best_inner,\n",
        "        knn_std_cv_score,\n",
        "        kg_best_inner,\n",
        "        kr_best_inner\n",
        "    ],\n",
        "    \"Time (s)\": [\n",
        "        dummy_time,\n",
        "        tree_time,\n",
        "        tree_grid_time,\n",
        "        tree_random_time,\n",
        "        knn_std_time,\n",
        "        knn_grid_time,\n",
        "        knn_random_time\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"Model Comparison Results:\")\n",
        "print(results_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection\n",
        "Our overall best performing classifier taking time into consideration was the grid search KNN, although this could be due simply to it arriving at the same parameters as the random search model. Regardless, we decided to take keep this model and use it to make our subsequent predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grid-Search KNN - Accuracy: 0.95, Balanced Accuracy: 0.90\n"
          ]
        }
      ],
      "source": [
        "# Outer evaluation\n",
        "test_predictions = grid_search_knn.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, test_predictions)\n",
        "test_bal_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Grid-Search KNN - Accuracy: {test_acc:.2f}, Balanced Accuracy: {test_bal_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We expect an accuracy ranging from 0.95 to 0.90 for our competition data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_competition = pd.read_csv('attrition_competition_02.csv')\n",
        "final_model = grid_search_knn.fit(X,y)\n",
        "competition_predictions = final_model.predict(X_competition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['final_model_AG_SQ.joblib']"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write the DataFrame to a .csv file\n",
        "# pd.DataFrame(competition_predictions).to_csv(\"competition_predictions.csv\", index=False)\n",
        "# import joblib\n",
        "# joblib.dump(final_model, 'final_model_AG_SQ.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The XGBClassifier or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'super' object has no attribute '__sklearn_tags__'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m xgb_pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Predict the target variable for the test dataset.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate accuracy and balanced accuracy to assess model performance.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
            "File \u001b[1;32mc:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:780\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform the data, and apply `predict` with the final estimator.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03mCall `transform` of each transformer in the pipeline. The transformed\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03m    Result of calling `predict` on the final estimator.\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;66;03m# TODO(1.8): Remove the context manager and use check_is_fitted(self)\u001b[39;00m\n\u001b[1;32m--> 780\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_raise_or_warn_if_not_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_routing_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:60\u001b[0m, in \u001b[0;36m_raise_or_warn_if_not_fitted\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# we only get here if the above didn't raise\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError:\n\u001b[0;32m     62\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis Pipeline instance is not fitted yet. Call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappropriate arguments before using other methods such as transform, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1756\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mrequires_fit \u001b[38;5;129;01mand\u001b[39;00m attributes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_or_any\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
            "File \u001b[1;32mc:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1665\u001b[0m, in \u001b[0;36m_is_fitted\u001b[1;34m(estimator, attributes, all_or_any)\u001b[0m\n\u001b[0;32m   1662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_or_any([\u001b[38;5;28mhasattr\u001b[39m(estimator, attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attributes])\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_is_fitted__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_is_fitted__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1667\u001b[0m fitted_attrs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1668\u001b[0m     v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1669\u001b[0m ]\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fitted_attrs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:1310\u001b[0m, in \u001b[0;36mPipeline.__sklearn_is_fitted__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1306\u001b[0m     \u001b[38;5;66;03m# check if the last step of the pipeline is fitted\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;66;03m# we only check the last step since if the last step is fit, it\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;66;03m# means the previous steps should also be fit. This is faster than\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;66;03m# checking if every step of the pipeline is fit.\u001b[39;00m\n\u001b[1;32m-> 1310\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError:\n",
            "File \u001b[1;32mc:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1751\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m-> 1751\u001b[0m tags \u001b[38;5;241m=\u001b[39m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mrequires_fit \u001b[38;5;129;01mand\u001b[39;00m attributes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_tags.py:405\u001b[0m, in \u001b[0;36mget_tags\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39mmro()):\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[1;32m--> 405\u001b[0m         sklearn_tags_provider[klass] \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_tags__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         class_order\u001b[38;5;241m.\u001b[39mappend(klass)\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_more_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
            "File \u001b[1;32mc:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:540\u001b[0m, in \u001b[0;36mClassifierMixin.__sklearn_tags__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_tags__\u001b[49m()\n\u001b[0;32m    541\u001b[0m     tags\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m     tags\u001b[38;5;241m.\u001b[39mclassifier_tags \u001b[38;5;241m=\u001b[39m ClassifierTags()\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# This model uses default hyperparameters with support for categorical variables enabled.\n",
        "xgb_classifier = xgb.XGBClassifier(\n",
        "    n_estimators=100,  # Number of trees (boosting rounds)\n",
        "    objective='binary:logistic',  # Binary classification\n",
        "    tree_method='hist',\n",
        "    eta=0.1,  # Learning rate to control the contribution of each tree\n",
        "    max_depth=3,  # Maximum depth of each tree\n",
        "    enable_categorical=True,\n",
        "    random_state=seed  # Fixed seed for reproducibility\n",
        ")\n",
        "\n",
        "xgb_pipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing\n",
        "    ('classifier', xgb_classifier)  # Train a KNN classifier\n",
        "])\n",
        "\n",
        "# Fit the model to the training data.\n",
        "xgb_pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target variable for the test dataset.\n",
        "y_pred = xgb_pipe.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and balanced accuracy to assess model performance.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Display basic results\n",
        "print(\"XGBoost Optimized with Default Hyperparameters and Categorical Support:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")  # Standard accuracy metric\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")  # Balanced metric for imbalanced datasets\n",
        "\n",
        "\n",
        "# Provides detailed performance metrics (precision, recall, F1-score) for each class.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Confusion matrix visualization for true vs predicted classes.\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb_classifier.classes_)\n",
        "disp.plot(cmap='viridis')  # Use a visually appealing colormap\n",
        "plt.title(\"Confusion Matrix for XGBoost\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPBeW4tOvSZ25FoTvU2ULzo",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
