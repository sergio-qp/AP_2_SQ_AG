{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML methods in scikit learn, using trees and knn, youre gonna need to encode categorical variables into numbers, for task 2!\n",
    "# you choose 1 dataset per group, add last 2 digits, that's the number for the dataset\n",
    "# add up 2 digits of 1 of our members' last student numbers, can double up with other groups, no biggie\n",
    "# available data(all inputs and output data) vs computational/competition(?) data(only input features). \n",
    "# scikit learn tutorials a bit up in aula global\n",
    "# use joblib library for... something in assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and cross validate different HP values for ur machine learning models, 3fold cross validation to evaluate all different models\n",
    "Feature selection is optional now, can be done for extra points... and so will be done\n",
    "Knn requires scaling ur data\n",
    "Tree based methods dont need scaling\n",
    "Neural networks do need scaling\n",
    "All preprocessing available in scikit learn\n",
    "minmax scaler function\n",
    "Decide best scaling method\n",
    "different scalers have different known issues better to just try all 3 and see what happens\n",
    "Better to do it using pipelines, a bit innefficient, but useful in general\n",
    "Other options are minmax or robust scalers then evaluate them against each other\n",
    "lowest RMSE is our best performing model, that's the one we pick\n",
    "\n",
    "Testing partition only for final evaluation, can't be contaminated by anything, DONT TOUCH IT UNTIL THE END, or ull fuck it up\n",
    "Training is selecting ur best alternative by using partitions and then u use ur final model on ur real data\n",
    "\n",
    "Create pipeline for preprocessing data and model training, REVIEW THIS IN FIRST PART OF LAST SLIDES\n",
    "\n",
    "Tutorial on aula global for how to do feature selection, for extra points!\n",
    "Attrition is burnout, that's ur resopnse variable (if thats ur data)\n",
    "\n",
    "FOR THE OPEN CHOICE PART, YOU CAN TRY DOING AN ENSEMBLE TECHNIQUE\n",
    "combine knn with like a random forest or some shit like that, idk\n",
    "\n",
    "gradient boosting is another technique, for trees, i think?\n",
    "External library: xgboost and catboost, and ligbm(?)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
