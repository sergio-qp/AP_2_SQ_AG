{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergio-qp/AP_2_SQ_AG/blob/master/Proyect_2_Draft%2003_01_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tX9VgN1lJq2V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from google.colab import files\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "qgmI1o4vAX-O",
        "outputId": "82291ae2-0cef-433e-e0df-bf0a46047f67"
      },
      "outputs": [],
      "source": [
        "\n",
        "# uploaded=files.upload()\n",
        "# file_name = list(uploaded.keys())[0]\n",
        "file_name = \"attrition_availabledata_02.csv\"\n",
        "data = pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z7tc1Hjl-OwF",
        "outputId": "608670ca-99e7-4312-90ce-ca049fad0d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         hrs  absences  JobInvolvement  PerformanceRating  \\\n",
            "0  10.060048       6.0             3.0                4.0   \n",
            "1   9.437671       2.0             2.0                3.0   \n",
            "2   7.900932      20.0             3.0                4.0   \n",
            "3   7.193853      19.0             4.0                3.0   \n",
            "4   6.979201       8.0             3.0                3.0   \n",
            "\n",
            "   EnvironmentSatisfaction  JobSatisfaction  WorkLifeBalance   Age  \\\n",
            "0                      2.0              4.0              1.0  31.0   \n",
            "1                      3.0              4.0              3.0  33.0   \n",
            "2                      3.0              4.0              3.0  35.0   \n",
            "3                      4.0              2.0              3.0  28.0   \n",
            "4                      2.0              4.0              2.0  31.0   \n",
            "\n",
            "      BusinessTravel              Department  ...  Over18  PercentSalaryHike  \\\n",
            "0  Travel_Frequently  Research & Development  ...       Y               23.0   \n",
            "1         Non-Travel  Research & Development  ...       Y               13.0   \n",
            "2      Travel_Rarely  Research & Development  ...       Y               22.0   \n",
            "3      Travel_Rarely  Research & Development  ...       Y               15.0   \n",
            "4      Travel_Rarely  Research & Development  ...       Y               12.0   \n",
            "\n",
            "  StandardHours  StockOptionLevel  TotalWorkingYears TrainingTimesLastYear  \\\n",
            "0           8.0               1.0                7.0                   5.0   \n",
            "1           8.0               0.0                7.0                   6.0   \n",
            "2           8.0               1.0               10.0                   4.0   \n",
            "3           8.0               0.0                1.0                   1.0   \n",
            "4           8.0               1.0               10.0                   2.0   \n",
            "\n",
            "   YearsAtCompany YearsSinceLastPromotion YearsWithCurrManager  Attrition  \n",
            "0             2.0                     2.0                  2.0        Yes  \n",
            "1             6.0                     1.0                  2.0         No  \n",
            "2            10.0                     7.0                  7.0        Yes  \n",
            "3             1.0                     0.0                  0.0         No  \n",
            "4             8.0                     7.0                  7.0         No  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_dnEXuqC5jN",
        "outputId": "4f4024be-9f9c-4977-add5-6f9a00784299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constant Columns Removed: []\n",
            "Number of Rows: 2940\n",
            "Number of Columns: 28\n",
            "Column Types: {'float64': 21, 'object': 7}\n",
            "Categorical Variables: ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Attrition']\n",
            "Numerical Variables: ['hrs', 'absences', 'JobInvolvement', 'PerformanceRating', 'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age', 'DistanceFromHome', 'Education', 'EmployeeID', 'JobLevel', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
            "Categorical Variables Table:          Variable  Cardinality\n",
            "0  BusinessTravel            3\n",
            "1      Department            3\n",
            "2  EducationField            6\n",
            "3          Gender            2\n",
            "4         JobRole            9\n",
            "5   MaritalStatus            3\n",
            "6       Attrition            2\n",
            "Missing Values: There are no missing values\n",
            "Removed Constant Columns: []\n",
            "Possible ID Columns: ['EmployeeID']\n",
            "Problem Type: classification\n",
            "Class Distribution (if classification): Attrition\n",
            "No     0.838776\n",
            "Yes    0.161224\n",
            "Name: count, dtype: float64\n",
            "Is Imbalanced: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# General Information\n",
        "num_rows = len(data)  # Number of rows\n",
        "num_columns = len(data.columns)  # Number of columns\n",
        "\n",
        "# Column Types\n",
        "column_types = data.dtypes  # Data types of the columns\n",
        "column_info = {}\n",
        "for dtype in column_types.unique():\n",
        "    column_info[str(dtype)] = sum(column_types == dtype)\n",
        "\n",
        "# Identify categorical and numerical variables\n",
        "categorical_vars = [col for col in data.columns if data[col].dtype == 'object']\n",
        "numerical_vars = [col for col in data.columns if data[col].dtype in ['int64', 'float64']]\n",
        "\n",
        "# Cardinality of categorical variables\n",
        "categorical_cardinality = {}\n",
        "for col in categorical_vars:\n",
        "    categorical_cardinality[col] = data[col].nunique()\n",
        "\n",
        "# Missing Values\n",
        "if data.isnull().sum().sum() > 0:\n",
        "    missing_values = \"There are missing values\"\n",
        "else:\n",
        "    missing_values = \"There are no missing values\"\n",
        "\n",
        "# Constant Columns\n",
        "constant_columns = [col for col in data.columns if data[col].nunique() == 1]\n",
        "print(f\"Constant Columns Removed: {constant_columns}\")\n",
        "\n",
        "# ID Columns\n",
        "possible_id_columns = []\n",
        "for col in data.columns:\n",
        "    if data[col].nunique() == num_rows:\n",
        "        possible_id_columns.append(col)\n",
        "\n",
        "# Problem Type (Regression or Classification)\n",
        "target_variable = data.columns[-1]  # Last column as target\n",
        "if data[target_variable].nunique() <= 10:\n",
        "    target_type = 'classification'\n",
        "else:\n",
        "    target_type = 'regression'\n",
        "\n",
        "# Class Distribution (if classification)\n",
        "if target_type == 'classification':\n",
        "    class_counts = data[target_variable].value_counts()  # Count each class\n",
        "    total_counts = len(data[target_variable])  # Total rows\n",
        "    class_distribution = class_counts / total_counts  # Proportion of each class\n",
        "\n",
        "    # Check if the dataset is imbalanced\n",
        "    is_imbalanced = class_distribution.max() > 0.6  # Use .max() with parentheses\n",
        "else:\n",
        "    class_distribution = None\n",
        "    is_imbalanced = None\n",
        "\n",
        "# EDA Summary\n",
        "eda_summary = {\n",
        "    \"Number of Rows\": num_rows,\n",
        "    \"Number of Columns\": num_columns,\n",
        "    \"Column Types\": column_info,\n",
        "    \"Categorical Variables\": categorical_vars,\n",
        "    \"Numerical Variables\": numerical_vars,\n",
        "    \"Categorical Variables Table\": pd.DataFrame({\n",
        "        \"Variable\": categorical_vars,\n",
        "        \"Cardinality\": [categorical_cardinality[col] for col in categorical_vars]\n",
        "    }),\n",
        "    \"Missing Values\": missing_values,\n",
        "    \"Removed Constant Columns\": constant_columns,\n",
        "    \"Possible ID Columns\": possible_id_columns,\n",
        "    \"Problem Type\": target_type,\n",
        "    \"Class Distribution (if classification)\": class_distribution,\n",
        "    \"Is Imbalanced\": is_imbalanced,\n",
        "}\n",
        "\n",
        "for key, value in eda_summary.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PdAQKikygqn"
      },
      "source": [
        "The dataset contains 2940 rows and 31 columns. Of these 31 variables, 23 are numerical variables, and 8 are categorical variables. These categorical variables include: BusinessTravel, Department, or Gender. They have different values; for example, Gender has two unique values: \"Female\" and \"Male.\" On the other hand, there are other categorical variables, such as JobRole, which have 9 unique values, indicating different roles like Sales Executive or Manager.\n",
        "\n",
        "Before performing any statistical analysis, it is crucial to check if the dataset contains missing values, as they can cause errors. As observed, the dataset does not have any missing values. Similarly, it is essential to verify if there are variables that have the same value across all observations, as these variables could be removed. In this dataset, Over18, EmployeeCount, and StandardHours are constant variables, which do not provide any useful information and should be eliminated.\n",
        "\n",
        "To determine whether we have a classification or regression problem, it is necessary to identify our target variable. The target variable is Attrition, whose unique values are \"Yes\" and \"No.\" Therefore, as it is a categorical variable, this is a classification problem.\n",
        "\n",
        "The target variable presents significant imbalance, with 83% of the observations corresponding to employees who do not leave (\"No\") and 16% belonging to the \"Yes\" category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofduxB81GLVa",
        "outputId": "230e69a8-46c7-4535-f84d-c0896503c81f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attrition\n",
            "0    2466\n",
            "1     474\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sergi\\AppData\\Local\\Temp\\ipykernel_1848\\1901188500.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y = data['Attrition'].replace({'No': 0, 'Yes': 1})\n"
          ]
        }
      ],
      "source": [
        "#SET UP\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop(columns=['Attrition'])  # Drop target variable\n",
        "y = data['Attrition'].replace({'No': 0, 'Yes': 1})\n",
        "print(y.value_counts())\n",
        "\n",
        "# Division de los datos\n",
        "# Split the dataset into training and testing sets (80/20 split)\n",
        "seed = 100533387  # Set student id seed for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g3aXDZyQlW-"
      },
      "source": [
        "The Holdout method involves splitting the dataset into training and test sets. In this case, 80% of the observations were assigned to the training set, and 20% to the test set.\n",
        "\n",
        "Holdout was used because it simulates the model's effectiveness on completely new data. Additionally, since the target variable is imbalanced, it is essential to maintain the proportion of classes in both samples to ensure that one sample does not contain only one class of the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IN048_cfW3w",
        "outputId": "336876ba-e686-44e0-b8e1-ac3307b0406f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Identify categorical and numerical columns\n",
        "# This separates the features into categorical and numerical types for preprocessing.\n",
        "categorical_vars = X.select_dtypes(include=['object']).columns.tolist()  # List of categorical columns\n",
        "numerical_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()  # List of numerical columns\n",
        "\n",
        "# Step 2: Create a preprocessor\n",
        "# A ColumnTransformer is used to apply different transformations to categorical and numerical columns.\n",
        "# - Categorical: OneHotEncoder to convert categories into binary columns.\n",
        "# - Numerical: 'passthrough' means numerical columns are not transformed.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_vars),  # Transform categorical variables\n",
        "        ('num', 'passthrough', numerical_vars)  # Keep numerical variables as is\n",
        "    ]\n",
        ")\n",
        "\n",
        "# might be worth it to split between ordinal and non ordinal, review later\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 1: Dummy Classifier (Baseline)\n",
        "# A simple classifier that always predicts the most frequent class. This is used as a baseline for comparison.\n",
        "start_dummy = time.time()  # Start timing\n",
        "dummy_clf = DummyClassifier(strategy=\"most_frequent\", random_state=seed)  # Most frequent strategy\n",
        "dummy_clf.fit(X_train, y_train)  # Train on the training data\n",
        "dummy_preds = dummy_clf.predict(X_test)  # Predict on the test data\n",
        "dummy_acc = accuracy_score(y_test, dummy_preds)  # Calculate accuracy\n",
        "dummy_bal_acc = balanced_accuracy_score(y_test, dummy_preds)  # Calculate balanced accuracy\n",
        "dummy_time = time.time() - start_dummy  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 2: Decision Tree with Pipeline\n",
        "# Combines the preprocessor and a Decision Tree model into a single pipeline for clean and efficient workflow.\n",
        "start_tree = time.time()  # Start timing\n",
        "tree_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing to the data\n",
        "    ('classifier', DecisionTreeClassifier(random_state=seed))  # Train a Decision Tree classifier\n",
        "])\n",
        "tree_pipeline.fit(X_train, y_train)  # Train the pipeline\n",
        "tree_preds = tree_pipeline.predict(X_test)  # Predict using the trained pipeline\n",
        "tree_acc = accuracy_score(y_test, tree_preds)  # Calculate accuracy\n",
        "tree_bal_acc = balanced_accuracy_score(y_test, tree_preds)  # Calculate balanced accuracy\n",
        "tree_time = time.time() - start_tree  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 3: KNN with StandardScaler\n",
        "# Combines preprocessing, scaling (StandardScaler), and KNN model into a single pipeline.\n",
        "start_knn_std = time.time()  # Start timing\n",
        "knn_pipeline_std = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing\n",
        "    ('scaler', StandardScaler()),  # Scale features to have mean=0 and std=1\n",
        "    ('classifier', KNeighborsClassifier())  # Train a KNN classifier\n",
        "])\n",
        "knn_pipeline_std.fit(X_train, y_train)  # Train the pipeline\n",
        "knn_std_preds = knn_pipeline_std.predict(X_test)  # Predict using the trained pipeline\n",
        "knn_std_acc = accuracy_score(y_test, knn_std_preds)  # Calculate accuracy\n",
        "knn_std_bal_acc = balanced_accuracy_score(y_test, knn_std_preds)  # Calculate balanced accuracy\n",
        "knn_std_time = time.time() - start_knn_std  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 4: KNN with MinMaxScaler\n",
        "# Similar to the StandardScaler pipeline, but uses MinMaxScaler for scaling.\n",
        "start_knn_minmax = time.time()  # Start timing\n",
        "knn_pipeline_minmax = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing\n",
        "    ('scaler', MinMaxScaler()),  # Scale features to range [0, 1]\n",
        "    ('classifier', KNeighborsClassifier())  # Train a KNN classifier\n",
        "])\n",
        "knn_pipeline_minmax.fit(X_train, y_train)  # Train the pipeline\n",
        "knn_minmax_preds = knn_pipeline_minmax.predict(X_test)  # Predict using the trained pipeline\n",
        "knn_minmax_acc = accuracy_score(y_test, knn_minmax_preds)  # Calculate accuracy\n",
        "knn_minmax_bal_acc = balanced_accuracy_score(y_test, knn_minmax_preds)  # Calculate balanced accuracy\n",
        "knn_minmax_time = time.time() - start_knn_minmax  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results:\n",
            "Dummy Classifier - Accuracy: 0.84, Balanced Accuracy: 0.50, Time: 0.01s\n",
            "Decision Tree - Accuracy: 0.93, Balanced Accuracy: 0.88, Time: 0.07s\n",
            "KNN (StandardScaler) - Accuracy: 0.83, Balanced Accuracy: 0.64, Time: 0.41s\n",
            "KNN (MinMaxScaler) - Accuracy: 0.83, Balanced Accuracy: 0.64, Time: 0.04s\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Display results\n",
        "# Print the performance metrics (Accuracy and Balanced Accuracy) for all models.\n",
        "# Additionally, display the time taken for each model.\n",
        "print(\"Results:\")\n",
        "print(f\"Dummy Classifier - Accuracy: {dummy_acc:.2f}, Balanced Accuracy: {dummy_bal_acc:.2f}, Time: {dummy_time:.2f}s\")\n",
        "print(f\"Decision Tree - Accuracy: {tree_acc:.2f}, Balanced Accuracy: {tree_bal_acc:.2f}, Time: {tree_time:.2f}s\")\n",
        "print(f\"KNN (StandardScaler) - Accuracy: {knn_std_acc:.2f}, Balanced Accuracy: {knn_std_bal_acc:.2f}, Time: {knn_std_time:.2f}s\")\n",
        "print(f\"KNN (MinMaxScaler) - Accuracy: {knn_minmax_acc:.2f}, Balanced Accuracy: {knn_minmax_bal_acc:.2f}, Time: {knn_minmax_time:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21FJ8sL5heoy"
      },
      "source": [
        "For this section, the following procedures were carried out:\n",
        "\n",
        "Identification of categorical and numerical columns: As performed in the EDA section, categorical and numerical variables were identified.\n",
        "Transformation of categorical variables: Categorical variables were converted into binary variables using OneHotEncoder.\n",
        "Dummy Classifier: This model predicts the most frequent class in the dataset.\n",
        "Decision Tree and KNN: A Decision Tree model was implemented, as well as KNN, which compares distances between data points. For KNN, the scales of variables can significantly impact the model's performance. Therefore, two types of scaling methods were compared: StandardScaler and MinMaxScaler.\n",
        "\n",
        "The effectiveness of the various classification methods was calculated and compared. We observed that the decision tree classifier was the most accurate in both our metrics and, while not the fastest, was still within an acceptable range. For KNN, we observed similar accuracy scores between the two scalers, but the MinMaxScaler was substantially faster, and as such we selected this scaling model for KNN in our next steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HPO to get tuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Trees with Grid-Search\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, RandomizedSearchCV\n",
        "inner = KFold(n_splits=3, shuffle=True, random_state=seed) # set up our inner 3fold cv evaluation method\n",
        "# just a couple of arrays with different parameters to test, IMPORTANT: classifier__ needs to go in front to be used in pipeline\n",
        "param_grid_tree = {'classifier__max_depth': [10, 20, 30], 'classifier__min_samples_split': [2, 10, 20]} \n",
        "start = time.time()  # Start timing\n",
        "\n",
        "# we use our preexisting tree pipeline\n",
        "grid_search_tree = GridSearchCV(tree_pipeline,param_grid_tree, cv=inner, scoring='neg_root_mean_squared_error')\n",
        "grid_search_tree.fit(X_train, y_train)\n",
        "\n",
        "tg_best_inner = grid_search_tree.best_score_\n",
        "tg_best_hp = grid_search_tree.best_params_\n",
        "\n",
        "tree_grid_time = time.time() - start  # End timing\n",
        "\n",
        "y_pred = grid_search_tree.predict(X_test)\n",
        "tree_grid_acc = accuracy_score(y_test, y_pred)\n",
        "tree_grid_bal_acc = balanced_accuracy_score(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Trees with Random-Search\n",
        "\n",
        "# param distribution this time\n",
        "param_dist_tree = {'classifier__max_depth':  np.arange(10, 31, 1), 'classifier__min_samples_split': np.arange(2, 21, 1)} \n",
        "start = time.time()  # Start timing\n",
        "\n",
        "# we use our preexisting tree pipeline\n",
        "random_search_tree = RandomizedSearchCV(tree_pipeline,param_dist_tree, cv=inner, scoring='neg_root_mean_squared_error', n_iter=10, random_state=seed)\n",
        "random_search_tree.fit(X_train, y_train)\n",
        "\n",
        "tr_best_inner = random_search_tree.best_score_\n",
        "tr_best_hp = random_search_tree.best_params_\n",
        "tree_random_time = time.time() - start  # End timing\n",
        "\n",
        "y_pred = random_search_tree.predict(X_test)\n",
        "tree_rand_acc = accuracy_score(y_test, y_pred)\n",
        "tree_rand_bal_acc = balanced_accuracy_score(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN with Grid-Search\n",
        "\n",
        "param_grid_knn = {'classifier__n_neighbors': [3,5,7],\n",
        "                  'classifier__weights': [\"uniform\",\"distance\"]} \n",
        "# we use our preexisting tree pipeline\n",
        "start = time.time()  # Start timing\n",
        "\n",
        "grid_search_knn = GridSearchCV(knn_pipeline_minmax,param_grid_knn, cv=inner, scoring='neg_root_mean_squared_error')\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "\n",
        "kg_best_inner = grid_search_knn.best_score_\n",
        "kg_best_hp = grid_search_knn.best_params_\n",
        "knn_grid_time = time.time() - start  # End timing\n",
        "\n",
        "y_pred = grid_search_knn.predict(X_test)\n",
        "knn_grid_acc = accuracy_score(y_test, y_pred)\n",
        "knn_grid_bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN with Random-Search\n",
        "param_dist_knn = {'classifier__n_neighbors': np.arange(3, 9, 1),\n",
        "                  'classifier__weights': [\"uniform\",\"distance\"]} \n",
        "start = time.time()  # Start timing\n",
        "\n",
        "# we use our preexisting tree pipeline\n",
        "random_search_knn = RandomizedSearchCV(knn_pipeline_minmax,param_dist_knn, cv=inner, scoring='neg_root_mean_squared_error', n_iter=10, random_state=seed)\n",
        "random_search_knn.fit(X_train, y_train)\n",
        "\n",
        "kr_best_inner = random_search_knn.best_score_\n",
        "kr_best_hp = random_search_knn.best_params_\n",
        "knn_random_time = time.time() - start  # End timing\n",
        "\n",
        "y_pred = random_search_knn.predict(X_test)\n",
        "knn_rand_acc = accuracy_score(y_test, y_pred)\n",
        "knn_rand_bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base Decision trees and KNN\n",
        "# use a dict for parameters and scores and such, to make it nicer later\n",
        "tree_default_scores = -cross_val_score(tree_pipeline, X_train, y_train,cv=inner, scoring='neg_root_mean_squared_error').mean()\n",
        "knn_default_scores = -cross_val_score(knn_pipeline_minmax, X_train, y_train,cv=inner, scoring='neg_root_mean_squared_error').mean()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results:\n",
            "Decision Trees (Base) - Inner RMSE: 0.33764997932927426, Best Hyperparameters: Default, Time: 0.06749296188354492s\n",
            "Decision Trees (Grid-Search) - Inner RMSE: 0.33092597435899024, Best Hyperparameters: {'classifier__max_depth': 10, 'classifier__min_samples_split': 2}, Time: 1.1858596801757812s\n",
            "Decision Trees (Random-Search) - Inner RMSE: 0.34461703606131566, Best Hyperparameters: {'classifier__min_samples_split': np.int64(4), 'classifier__max_depth': np.int64(20)}, Time: 1.335611343383789s\n",
            "KNN (Base) - Inner RMSE: 0.40824564041736683, Best Hyperparameters: Default, Time: 0.04199552536010742s\n",
            "KNN (Grid-Search) - Inner RMSE: 0.3212280569843986, Best Hyperparameters: {'classifier__n_neighbors': 3, 'classifier__weights': 'distance'}, Time: 0.8762192726135254s\n",
            "KNN (Random-Search) - Inner RMSE: 0.3212280569843986, Best Hyperparameters: {'classifier__weights': 'distance', 'classifier__n_neighbors': np.int64(3)}, Time: 1.3008394241333008s\n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "\n",
        "print(\"Results:\")\n",
        "print(f\"Decision Trees (Base) - Inner RMSE: {tree_default_scores}, Best Hyperparameters: Default, Time: {tree_time}s\")\n",
        "print(f\"Decision Trees (Grid-Search) - Inner RMSE: {-tg_best_inner}, Best Hyperparameters: {tg_best_hp}, Time: {tree_grid_time}s\")\n",
        "print(f\"Decision Trees (Random-Search) - Inner RMSE: {-tr_best_inner}, Best Hyperparameters: {tr_best_hp}, Time: {tree_random_time}s\")\n",
        "print(f\"KNN (Base) - Inner RMSE: {knn_default_scores}, Best Hyperparameters: Default, Time: {knn_minmax_time}s\")\n",
        "print(f\"KNN (Grid-Search) - Inner RMSE: {-kg_best_inner}, Best Hyperparameters: {kg_best_hp}, Time: {knn_grid_time}s\")\n",
        "print(f\"KNN (Random-Search) - Inner RMSE: {-kr_best_inner}, Best Hyperparameters: {kr_best_hp}, Time: {knn_random_time}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGCx49x_dRnZ"
      },
      "source": [
        "### Decision Trees\n",
        "We observed a slight improvement over the base parameters (`RMSE: 0.337`) using grid search (`RMSE: 0.331`), but a decrease in performance when using random search (`RMSE: 0.344`). Given the comparatively larger depth and min split in the random example, it appears to be a case of overfitting that led to a performance poorer than the base classifier. As such, within decision trees, the grid search tuned model with a max depth of 10 and min split of 2 performed the best.\n",
        "\n",
        "### KNN\n",
        "In the case of KNN, both models returned identical parameters (`RMSE: 0.321`) and therefore identical performance, although grid search was slightly faster. Both performed better than the base KNN classifier (`RMSE: 0.408`), which was the worst performing overall.\n",
        "\n",
        "### Summary\n",
        "Regarding execution speed, the tuning step greatly slowed the overall performance of all tuned models in comparison to their base versions. Grid search generally performed slightly faster than random search, which makes sense given that grid search depends on the amount of parameters input to be tested, which was generally around 3 per field, while random depended on the amount of iterations, which we kept at 10. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Comparison Results:\n",
            "                           Method  Accuracy  Balanced Accuracy  Time (s)\n",
            "0                Dummy Classifier  0.838435           0.500000  0.006999\n",
            "1            Decision Tree (Base)  0.931973           0.878702  0.067493\n",
            "2    Decision Trees (Grid-Search)  0.921769           0.821629  1.185860\n",
            "3  Decision Trees (Random-Search)  0.935374           0.884979  1.335611\n",
            "4                      KNN (Base)  0.829932           0.635145  0.041996\n",
            "5               KNN (Grid-Search)  0.945578           0.899562  0.876219\n",
            "6             KNN (Random-Search)  0.945578           0.899562  1.300839\n"
          ]
        }
      ],
      "source": [
        "\n",
        "results = {\n",
        "    \"Method\": [\n",
        "        \"Dummy Classifier\",\n",
        "        \"Decision Tree (Base)\",\n",
        "        \"Decision Trees (Grid-Search)\",\n",
        "        \"Decision Trees (Random-Search)\",\n",
        "        \"KNN (Base)\",\n",
        "        \"KNN (Grid-Search)\",\n",
        "        \"KNN (Random-Search)\"\n",
        "    ],\n",
        "    \"Accuracy\": [\n",
        "        dummy_acc,\n",
        "        tree_acc,\n",
        "        tree_grid_acc,\n",
        "        tree_rand_acc,\n",
        "        knn_minmax_acc,\n",
        "        knn_grid_acc,\n",
        "        knn_rand_acc\n",
        "    ],\n",
        "    \"Balanced Accuracy\": [\n",
        "        dummy_bal_acc,\n",
        "        tree_bal_acc,\n",
        "        tree_grid_bal_acc,\n",
        "        tree_rand_bal_acc,\n",
        "        knn_minmax_bal_acc,\n",
        "        knn_grid_bal_acc,\n",
        "        knn_rand_bal_acc\n",
        "    ],\n",
        "    \"Time (s)\": [\n",
        "        dummy_time,\n",
        "        tree_time,\n",
        "        tree_grid_time,\n",
        "        tree_random_time,\n",
        "        knn_minmax_time,\n",
        "        knn_grid_time,\n",
        "        knn_random_time\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"Model Comparison Results:\")\n",
        "print(results_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection\n",
        "Our overall best performing classifier taking time into consideration was the grid search KNN, although this could be due simply to it arriving at the same parameters as the random search model. Regardless, we decided to take keep this model and use it to make our subsequent predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grid-Search KNN - Accuracy: 1.00, Balanced Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Outer evaluation\n",
        "test_predictions = grid_search_knn.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, test_predictions)\n",
        "test_bal_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Grid-Search KNN - Accuracy: {test_acc:.2f}, Balanced Accuracy: {test_bal_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We expect an accuracy ranging from 0.95 to 0.90 for our competition data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_competition = pd.read_csv('attrition_competition_02.csv')\n",
        "final_model = grid_search_knn.fit(X,y)\n",
        "competition_predictions = final_model.predict(X_competition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['final_model_AG_SQ.joblib']"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write the DataFrame to a .csv file\n",
        "# pd.DataFrame(competition_predictions).to_csv(\"competition_predictions.csv\", index=False)\n",
        "# import joblib\n",
        "# joblib.dump(final_model, 'final_model_AG_SQ.joblib')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPBeW4tOvSZ25FoTvU2ULzo",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
