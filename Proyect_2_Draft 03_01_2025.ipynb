{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergio-qp/AP_2_SQ_AG/blob/master/Proyect_2_Draft%2003_01_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alejandro GarcÃ­a and Sergio Quintanilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "tX9VgN1lJq2V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from google.colab import files\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "This project aims to predict whether employees will leave the company using machine learning techniques.\n",
        "\n",
        "We explored various models, from baseline methods like Dummy Classifiers to advanced techniques like XGBoost. Key preprocessing steps, including scaling, encoding, and handling class imbalance, were implemented to ensure robust performance. Metrics such as accuracy, balanced accuracy, and F1-score were used to evaluate the models.\n",
        "\n",
        "By combining predictive power and interpretability, this project provides insights into employee attrition, helping organizations make informed decisions to retain their workforce effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "qgmI1o4vAX-O",
        "outputId": "82291ae2-0cef-433e-e0df-bf0a46047f67"
      },
      "outputs": [],
      "source": [
        "\n",
        "# uploaded=files.upload()\n",
        "# file_name = list(uploaded.keys())[0]\n",
        "file_name = \"attrition_availabledata_02.csv\"\n",
        "data = pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z7tc1Hjl-OwF",
        "outputId": "608670ca-99e7-4312-90ce-ca049fad0d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         hrs  absences  JobInvolvement  PerformanceRating  \\\n",
            "0  10.060048       6.0             3.0                4.0   \n",
            "1   9.437671       2.0             2.0                3.0   \n",
            "2   7.900932      20.0             3.0                4.0   \n",
            "3   7.193853      19.0             4.0                3.0   \n",
            "4   6.979201       8.0             3.0                3.0   \n",
            "\n",
            "   EnvironmentSatisfaction  JobSatisfaction  WorkLifeBalance   Age  \\\n",
            "0                      2.0              4.0              1.0  31.0   \n",
            "1                      3.0              4.0              3.0  33.0   \n",
            "2                      3.0              4.0              3.0  35.0   \n",
            "3                      4.0              2.0              3.0  28.0   \n",
            "4                      2.0              4.0              2.0  31.0   \n",
            "\n",
            "      BusinessTravel              Department  ...  Over18  PercentSalaryHike  \\\n",
            "0  Travel_Frequently  Research & Development  ...       Y               23.0   \n",
            "1         Non-Travel  Research & Development  ...       Y               13.0   \n",
            "2      Travel_Rarely  Research & Development  ...       Y               22.0   \n",
            "3      Travel_Rarely  Research & Development  ...       Y               15.0   \n",
            "4      Travel_Rarely  Research & Development  ...       Y               12.0   \n",
            "\n",
            "  StandardHours  StockOptionLevel  TotalWorkingYears TrainingTimesLastYear  \\\n",
            "0           8.0               1.0                7.0                   5.0   \n",
            "1           8.0               0.0                7.0                   6.0   \n",
            "2           8.0               1.0               10.0                   4.0   \n",
            "3           8.0               0.0                1.0                   1.0   \n",
            "4           8.0               1.0               10.0                   2.0   \n",
            "\n",
            "   YearsAtCompany YearsSinceLastPromotion YearsWithCurrManager  Attrition  \n",
            "0             2.0                     2.0                  2.0        Yes  \n",
            "1             6.0                     1.0                  2.0         No  \n",
            "2            10.0                     7.0                  7.0        Yes  \n",
            "3             1.0                     0.0                  0.0         No  \n",
            "4             8.0                     7.0                  7.0         No  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_dnEXuqC5jN",
        "outputId": "4f4024be-9f9c-4977-add5-6f9a00784299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constant Columns Removed: ['EmployeeCount', 'Over18', 'StandardHours']\n",
            "Number of Rows: 2940\n",
            "Number of Columns: 31\n",
            "Column Types: {'float64': 23, 'object': 8}\n",
            "Categorical Variables: ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'Attrition']\n",
            "Numerical Variables: ['hrs', 'absences', 'JobInvolvement', 'PerformanceRating', 'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age', 'DistanceFromHome', 'Education', 'EmployeeCount', 'EmployeeID', 'JobLevel', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
            "Categorical Variables Table:          Variable  Cardinality\n",
            "0  BusinessTravel            3\n",
            "1      Department            3\n",
            "2  EducationField            6\n",
            "3          Gender            2\n",
            "4         JobRole            9\n",
            "5   MaritalStatus            3\n",
            "6          Over18            1\n",
            "7       Attrition            2\n",
            "Missing Values: There are no missing values\n",
            "Removed Constant Columns: ['EmployeeCount', 'Over18', 'StandardHours']\n",
            "Possible ID Columns: ['EmployeeID']\n",
            "Problem Type: classification\n",
            "Class Distribution (if classification): Attrition\n",
            "No     0.838776\n",
            "Yes    0.161224\n",
            "Name: count, dtype: float64\n",
            "Is Imbalanced: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# General Information\n",
        "num_rows = len(data)  # Number of rows\n",
        "num_columns = len(data.columns)  # Number of columns\n",
        "\n",
        "# Column Types\n",
        "column_types = data.dtypes  # Data types of the columns\n",
        "column_info = {}\n",
        "for dtype in column_types.unique():\n",
        "    column_info[str(dtype)] = sum(column_types == dtype)\n",
        "\n",
        "# Identify categorical and numerical variables\n",
        "categorical_vars = [col for col in data.columns if data[col].dtype == 'object']\n",
        "numerical_vars = [col for col in data.columns if data[col].dtype in ['int64', 'float64']]\n",
        "\n",
        "# Cardinality of categorical variables\n",
        "categorical_cardinality = {}\n",
        "for col in categorical_vars:\n",
        "    categorical_cardinality[col] = data[col].nunique()\n",
        "\n",
        "# Missing Values\n",
        "if data.isnull().sum().sum() > 0:\n",
        "    missing_values = \"There are missing values\"\n",
        "else:\n",
        "    missing_values = \"There are no missing values\"\n",
        "\n",
        "# Constant Columns\n",
        "constant_columns = [col for col in data.columns if data[col].nunique() == 1]\n",
        "print(f\"Constant Columns Removed: {constant_columns}\")\n",
        "\n",
        "# ID Columns\n",
        "possible_id_columns = []\n",
        "for col in data.columns:\n",
        "    if data[col].nunique() == num_rows:\n",
        "        possible_id_columns.append(col)\n",
        "\n",
        "# Problem Type (Regression or Classification)\n",
        "target_variable = data.columns[-1]  # Last column as target\n",
        "if data[target_variable].nunique() <= 10:\n",
        "    target_type = 'classification'\n",
        "else:\n",
        "    target_type = 'regression'\n",
        "\n",
        "# Class Distribution (if classification)\n",
        "if target_type == 'classification':\n",
        "    class_counts = data[target_variable].value_counts()  # Count each class\n",
        "    total_counts = len(data[target_variable])  # Total rows\n",
        "    class_distribution = class_counts / total_counts  # Proportion of each class\n",
        "\n",
        "    # Check if the dataset is imbalanced\n",
        "    is_imbalanced = class_distribution.max() > 0.6  # Use .max() with parentheses\n",
        "else:\n",
        "    class_distribution = None\n",
        "    is_imbalanced = None\n",
        "\n",
        "# EDA Summary\n",
        "eda_summary = {\n",
        "    \"Number of Rows\": num_rows,\n",
        "    \"Number of Columns\": num_columns,\n",
        "    \"Column Types\": column_info,\n",
        "    \"Categorical Variables\": categorical_vars,\n",
        "    \"Numerical Variables\": numerical_vars,\n",
        "    \"Categorical Variables Table\": pd.DataFrame({\n",
        "        \"Variable\": categorical_vars,\n",
        "        \"Cardinality\": [categorical_cardinality[col] for col in categorical_vars]\n",
        "    }),\n",
        "    \"Missing Values\": missing_values,\n",
        "    \"Removed Constant Columns\": constant_columns,\n",
        "    \"Possible ID Columns\": possible_id_columns,\n",
        "    \"Problem Type\": target_type,\n",
        "    \"Class Distribution (if classification)\": class_distribution,\n",
        "    \"Is Imbalanced\": is_imbalanced,\n",
        "}\n",
        "\n",
        "for key, value in eda_summary.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PdAQKikygqn"
      },
      "source": [
        "The dataset contains 2940 rows and 31 columns. Of these 31 variables, 23 are numerical variables, and 8 are categorical variables. These categorical variables include: BusinessTravel, Department, or Gender. They have different values; for example, Gender has two unique values: \"Female\" and \"Male.\" On the other hand, there are other categorical variables, such as JobRole, which have 9 unique values, indicating different roles like Sales Executive or Manager.\n",
        "\n",
        "Before performing any statistical analysis, it is crucial to check if the dataset contains missing values, as they can cause errors. As observed, the dataset does not have any missing values. Similarly, it is essential to verify if there are variables that have the same value across all observations, as these variables could be removed. In this dataset, Over18, EmployeeCount, and StandardHours are constant variables, which do not provide any useful information and should be eliminated.\n",
        "\n",
        "To determine whether we have a classification or regression problem, it is necessary to identify our target variable. The target variable is Attrition, whose unique values are \"Yes\" and \"No.\" Therefore, as it is a categorical variable, this is a classification problem.\n",
        "\n",
        "The target variable presents significant imbalance, with 83% of the observations corresponding to employees who do not leave (\"No\") and 16% belonging to the \"Yes\" category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofduxB81GLVa",
        "outputId": "230e69a8-46c7-4535-f84d-c0896503c81f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attrition\n",
            "0    2466\n",
            "1     474\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sergi\\AppData\\Local\\Temp\\ipykernel_25424\\1982002934.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y = data['Attrition'].replace({'No': 0, 'Yes': 1})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop(columns=['Attrition'])  # Drop target variable\n",
        "y = data['Attrition'].replace({'No': 0, 'Yes': 1})\n",
        "print(y.value_counts())\n",
        "\n",
        "# Division de los datos\n",
        "# Split the dataset into training and testing sets (80/20 split)\n",
        "seed = 100533387  # Set student id seed for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g3aXDZyQlW-"
      },
      "source": [
        "We split our data 80/20 into train and test sets so we can use them for the Holdout method when we want to estimate future performance later.\n",
        "\n",
        "For inner evaluation, we used 3-fold StratifiedKFold since the target variable is imbalanced. This way, we maintain the proportion of classes in our samples to ensure that one sample does not contain only one class of the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IN048_cfW3w",
        "outputId": "336876ba-e686-44e0-b8e1-ac3307b0406f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Identify categorical and numerical columns\n",
        "# This separates the features into categorical and numerical types for preprocessing.\n",
        "categorical_vars = X.select_dtypes(include=['object']).columns.tolist()  # List of categorical columns\n",
        "numerical_vars = X.select_dtypes(include=['int64', 'float64']).columns.tolist()  # List of numerical columns\n",
        "\n",
        "# Step 2: Create a preprocessor\n",
        "# A ColumnTransformer is used to apply different transformations to categorical and numerical columns.\n",
        "# - Categorical: OneHotEncoder to convert categories into binary columns.\n",
        "# - Numerical: 'passthrough' means numerical columns are not transformed.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_vars),  # Transform categorical variables\n",
        "        ('num', 'passthrough', numerical_vars)  # Keep numerical variables as is\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
        "inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed) # set up our inner 3fold cv evaluation method\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 1: Dummy Classifier (Baseline)\n",
        "# A simple classifier that always predicts the most frequent class. This is used as a baseline for comparison.\n",
        "start_dummy = time.time()  # Start timing\n",
        "dummy_clf = DummyClassifier(strategy=\"most_frequent\", random_state=seed)  # Most frequent strategy\n",
        "dummy_clf.fit(X_train, y_train)  # Train on the training data\n",
        "dummy_cv_score = -cross_val_score(dummy_clf, X_train, y_train,cv=inner, scoring='neg_root_mean_squared_error').mean() # Get our 3fold CV score\n",
        "dummy_time = time.time() - start_dummy  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 2: Decision Tree with Pipeline\n",
        "# Combines the preprocessor and a Decision Tree model into a single pipeline for clean and efficient workflow.\n",
        "start_tree = time.time()  # Start timing\n",
        "tree_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing to the data\n",
        "    ('classifier', DecisionTreeClassifier(random_state=seed))  # Train a Decision Tree classifier\n",
        "])\n",
        "tree_pipeline.fit(X_train, y_train)  # Train the pipeline\n",
        "tree_cv_score = -cross_val_score(tree_pipeline, X_train, y_train,cv=inner, scoring='neg_root_mean_squared_error').mean() # Get our 3fold CV score\n",
        "tree_time = time.time() - start_tree  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 3: KNN with StandardScaler\n",
        "# Combines preprocessing, scaling (StandardScaler), and KNN model into a single pipeline.\n",
        "start_knn_std = time.time()  # Start timing\n",
        "knn_pipeline_std = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing\n",
        "    ('scaler', StandardScaler()),  # Scale features to have mean=0 and std=1\n",
        "    ('classifier', KNeighborsClassifier())  # Train a KNN classifier\n",
        "])\n",
        "knn_pipeline_std.fit(X_train, y_train)  # Train the pipeline\n",
        "knn_std_cv_score = -cross_val_score(knn_pipeline_std, X_train, y_train,cv=inner, scoring='neg_root_mean_squared_error').mean() # Get our 3fold CV score\n",
        "knn_std_time = time.time() - start_knn_std  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method 4: KNN with MinMaxScaler\n",
        "# Similar to the StandardScaler pipeline, but uses MinMaxScaler for scaling.\n",
        "start_knn_minmax = time.time()  # Start timing\n",
        "knn_pipeline_minmax = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing\n",
        "    ('scaler', MinMaxScaler()),  # Scale features to range [0, 1]\n",
        "    ('classifier', KNeighborsClassifier())  # Train a KNN classifier\n",
        "])\n",
        "knn_pipeline_minmax.fit(X_train, y_train)  # Train the pipeline\n",
        "knn_minmax_cv_score = -cross_val_score(knn_pipeline_minmax, X_train, y_train,cv=inner, scoring='neg_root_mean_squared_error').mean() # Get our 3fold CV score\n",
        "knn_minmax_time = time.time() - start_knn_minmax  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results:\n",
            "Dummy Classifier - RMSE: 0.40, Time: 0.01s\n",
            "Decision Tree - RMSE: 0.35, Time: 0.20s\n",
            "KNN (StandardScaler) - RMSE: 0.41, Time: 0.17s\n",
            "KNN (MinMaxScaler) - RMSE: 0.42, Time: 0.17s\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Display results\n",
        "# Print the performance metrics (Accuracy and Balanced Accuracy) for all models.\n",
        "# Additionally, display the time taken for each model.\n",
        "print(\"Results:\")\n",
        "print(f\"Dummy Classifier - RMSE: {dummy_cv_score:.2f}, Time: {dummy_time:.2f}s\")\n",
        "print(f\"Decision Tree - RMSE: {tree_cv_score:.2f}, Time: {tree_time:.2f}s\")\n",
        "print(f\"KNN (StandardScaler) - RMSE: {knn_std_cv_score:.2f}, Time: {knn_std_time:.2f}s\")\n",
        "print(f\"KNN (MinMaxScaler) - RMSE: {knn_minmax_cv_score:.2f}, Time: {knn_minmax_time:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21FJ8sL5heoy"
      },
      "source": [
        "For this section, the following procedures were carried out:\n",
        "\n",
        "Identification of categorical and numerical columns: As performed in the EDA section, categorical and numerical variables were identified.\n",
        "Transformation of categorical variables: Categorical variables were converted into binary variables using OneHotEncoder.\n",
        "Dummy Classifier: This model predicts the most frequent class in the dataset.\n",
        "Decision Tree and KNN: A Decision Tree model was implemented, as well as KNN, which compares distances between data points. For KNN, the scales of variables can significantly impact the model's performance. Therefore, two types of scaling methods were compared: StandardScaler and MinMaxScaler.\n",
        "\n",
        "The effectiveness of the various classification methods was calculated and compared. We observed that the dummy classifier was by far the fastest, while all the other models took around the same time as each other to execute. Given that the KNN standard scaler was slightly better than the minmax one and executed slightly faster, we decided to use this one in the following steps of our project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HPO to get tuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Trees with Grid-Search\n",
        "\n",
        "# just a couple of arrays with different parameters to test, IMPORTANT: classifier__ needs to go in front to be used in pipeline\n",
        "param_grid_tree = {'classifier__max_depth': [10, 20, 30], 'classifier__min_samples_split': [2, 10, 20]} \n",
        "start = time.time()  # Start timing\n",
        "\n",
        "# we use our preexisting tree pipeline\n",
        "grid_search_tree = GridSearchCV(tree_pipeline,param_grid_tree, cv=inner, scoring='neg_root_mean_squared_error')\n",
        "grid_search_tree.fit(X_train, y_train)\n",
        "\n",
        "tg_best_inner = -grid_search_tree.best_score_\n",
        "tg_best_hp = grid_search_tree.best_params_\n",
        "\n",
        "tree_grid_time = time.time() - start  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Trees with Random-Search\n",
        "\n",
        "# param distribution this time\n",
        "param_dist_tree = {'classifier__max_depth':  np.arange(10, 31, 1), 'classifier__min_samples_split': np.arange(2, 21, 1)} \n",
        "start = time.time()  # Start timing\n",
        "\n",
        "# we use our preexisting tree pipeline\n",
        "random_search_tree = RandomizedSearchCV(tree_pipeline,param_dist_tree, cv=inner, scoring='neg_root_mean_squared_error', n_iter=10, random_state=seed)\n",
        "random_search_tree.fit(X_train, y_train)\n",
        "\n",
        "tr_best_inner = -random_search_tree.best_score_\n",
        "tr_best_hp = random_search_tree.best_params_\n",
        "tree_random_time = time.time() - start  # End timing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN with Grid-Search\n",
        "\n",
        "param_grid_knn = {'classifier__n_neighbors': [3,5,7],\n",
        "                  'classifier__weights': [\"uniform\",\"distance\"]} \n",
        "# we use our preexisting tree pipeline\n",
        "start = time.time()  # Start timing\n",
        "\n",
        "grid_search_knn = GridSearchCV(knn_pipeline_std,param_grid_knn, cv=inner, scoring='neg_root_mean_squared_error')\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "\n",
        "kg_best_inner = -grid_search_knn.best_score_\n",
        "kg_best_hp = grid_search_knn.best_params_\n",
        "knn_grid_time = time.time() - start  # End timing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN with Random-Search\n",
        "param_dist_knn = {'classifier__n_neighbors': np.arange(3, 9, 1),\n",
        "                  'classifier__weights': [\"uniform\",\"distance\"]} \n",
        "start = time.time()  # Start timing\n",
        "\n",
        "# we use our preexisting tree pipeline\n",
        "random_search_knn = RandomizedSearchCV(knn_pipeline_std,param_dist_knn, cv=inner, scoring='neg_root_mean_squared_error', n_iter=10, random_state=seed)\n",
        "random_search_knn.fit(X_train, y_train)\n",
        "\n",
        "kr_best_inner = -random_search_knn.best_score_\n",
        "kr_best_hp = random_search_knn.best_params_\n",
        "knn_random_time = time.time() - start  # End timing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Comparison Results:\n",
            "                           Method  Inner RMSE  Time (s)  \\\n",
            "0                Dummy Classifier    0.401421  0.014030   \n",
            "1            Decision Tree (Base)    0.353991  0.203005   \n",
            "2    Decision Trees (Grid-Search)    0.350256  1.398474   \n",
            "3  Decision Trees (Random-Search)    0.343829  1.383564   \n",
            "4                      KNN (Base)    0.405610  0.167053   \n",
            "5               KNN (Grid-Search)    0.330475  0.907001   \n",
            "6             KNN (Random-Search)    0.330475  1.532112   \n",
            "\n",
            "                                     Hyperparameters  \n",
            "0                                                 NA  \n",
            "1                                            Default  \n",
            "2  {'classifier__max_depth': 10, 'classifier__min...  \n",
            "3  {'classifier__min_samples_split': 3, 'classifi...  \n",
            "4                                            Default  \n",
            "5  {'classifier__n_neighbors': 3, 'classifier__we...  \n",
            "6  {'classifier__weights': 'distance', 'classifie...  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "results = {\n",
        "    \"Method\": [\n",
        "        \"Dummy Classifier\",\n",
        "        \"Decision Tree (Base)\",\n",
        "        \"Decision Trees (Grid-Search)\",\n",
        "        \"Decision Trees (Random-Search)\",\n",
        "        \"KNN (Base)\",\n",
        "        \"KNN (Grid-Search)\",\n",
        "        \"KNN (Random-Search)\"\n",
        "    ],\n",
        "    \"Inner RMSE\": [\n",
        "        dummy_cv_score,\n",
        "        tree_cv_score,\n",
        "        tg_best_inner,\n",
        "        tr_best_inner,\n",
        "        knn_std_cv_score,\n",
        "        kg_best_inner,\n",
        "        kr_best_inner\n",
        "    ],\n",
        "    \"Time (s)\": [\n",
        "        dummy_time,\n",
        "        tree_time,\n",
        "        tree_grid_time,\n",
        "        tree_random_time,\n",
        "        knn_std_time,\n",
        "        knn_grid_time,\n",
        "        knn_random_time\n",
        "    ],\n",
        "    \"Hyperparameters\": [\n",
        "        \"NA\",\n",
        "        \"Default\",\n",
        "        tg_best_hp,\n",
        "        tr_best_hp,\n",
        "        \"Default\",\n",
        "        kg_best_hp,\n",
        "        kr_best_hp\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"Model Comparison Results:\")\n",
        "print(results_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGCx49x_dRnZ"
      },
      "source": [
        "### Decision Trees\n",
        "We observed a slight improvement over the base parameters (`RMSE: 0.353`) using grid search (`RMSE: 0.350`), and a larger increase in performance when using random search (`RMSE: 0.344`). The added flexibility in random search might have helped the model find more ideal parameters than with just grid search.\n",
        "\n",
        "### KNN\n",
        "In the case of KNN, both models returned identical parameters (`RMSE: 0.330`) and therefore identical performance, although grid search was slightly faster. Both performed better than the base KNN classifier (`RMSE: 0.406`), which was the worst performing overall.\n",
        "\n",
        "### Summary\n",
        "Regarding execution speed, the tuning step greatly slowed the overall performance of all tuned models in comparison to their base versions. Grid search generally performed slightly faster than random search in KNN, which makes sense given that grid search depends on the amount of parameters input to be tested, which was generally around 3 per field, while random depended on the amount of iterations, which we kept at 10. In trees, they performed about as quickly as each other.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection\n",
        "Our overall best performing classifier was the grid search KNN, although this could be due simply to it arriving at the same parameters as the random search model, albeit a little faster. Regardless, we decided to take keep this model and use it to make our subsequent predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grid-Search KNN - Accuracy: 0.95, Balanced Accuracy: 0.90\n"
          ]
        }
      ],
      "source": [
        "# Outer evaluation\n",
        "test_predictions = grid_search_knn.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, test_predictions)\n",
        "test_bal_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Grid-Search KNN - Accuracy: {test_acc:.2f}, Balanced Accuracy: {test_bal_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We expect an accuracy ranging from 0.95 to 0.90 for our competition data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_competition = pd.read_csv('attrition_competition_02.csv')\n",
        "final_model = grid_search_knn.fit(X,y)\n",
        "competition_predictions = final_model.predict(X_competition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the DataFrame to a .csv file\n",
        "# pd.DataFrame(competition_predictions).to_csv(\"competition_predictions.csv\", index=False)\n",
        "# import joblib\n",
        "# joblib.dump(final_model, 'final_model_AG_SQ.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "# Using a chatgpt wrapper to fix the compatibility issues\n",
        "\n",
        "class XGBClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Initialize the XGBClassifier with passed arguments\n",
        "        self.model = xgb.XGBClassifier(*args, **kwargs)\n",
        "\n",
        "    def fit(self, X, y, **kwargs):\n",
        "        # Fit the model\n",
        "        return self.model.fit(X, y, **kwargs)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict the target labels\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Predict probabilities for the classes\n",
        "        return self.model.predict_proba(X)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # Get model parameters\n",
        "        return self.model.get_params(deep)\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        # Set model parameters\n",
        "        self.model.set_params(**params)\n",
        "\n",
        "    @property\n",
        "    def classes_(self):\n",
        "        # Return the classes learned by the model\n",
        "        return self.model.classes_\n",
        "\n",
        "    # Explicitly define the sklearn tags for this model\n",
        "    @property\n",
        "    def _sklearn_tags(self):\n",
        "        # This is the fix for the error you're encountering\n",
        "        return {\n",
        "            'estimator_type': 'classifier',\n",
        "            'requires_y': True,  # XGBClassifier needs labels for classification\n",
        "            'binary_only': True,  # This is a binary classifier (for binary classification)\n",
        "            'class_weight': None,\n",
        "            'multi_class': False\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sergi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Optimized with Default Hyperparameters and Categorical Support:\n",
            "Accuracy: 0.89\n",
            "Balanced Accuracy: 0.70\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.99      0.94       493\n",
            "           1       0.85      0.41      0.55        95\n",
            "\n",
            "    accuracy                           0.89       588\n",
            "   macro avg       0.87      0.70      0.75       588\n",
            "weighted avg       0.89      0.89      0.88       588\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# This model uses default hyperparameters with support for categorical variables enabled.\n",
        "xgb_pipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),  # Apply preprocessing\n",
        "    ('classifier', XGBClassifierWrapper(n_estimators=100, max_depth=3, eta=0.1, random_state=42, enable_categorical=True))\n",
        "])\n",
        "\n",
        "# Fit the model to the training data.\n",
        "xgb_pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target variable for the test dataset.\n",
        "y_pred = xgb_pipe.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and balanced accuracy to assess model performance.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Display basic results\n",
        "print(\"XGBoost Optimized with Default Hyperparameters and Categorical Support:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")  # Standard accuracy metric\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")  # Balanced metric for imbalanced datasets\n",
        "\n",
        "\n",
        "# Provides detailed performance metrics (precision, recall, F1-score) for each class.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Confusion matrix visualization for true vs predicted classes.\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb_pipe.classes_)\n",
        "disp.plot(cmap='viridis')  # Use a visually appealing colormap\n",
        "plt.title(\"Confusion Matrix for XGBoost\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The decision to implement XGBoosting as an additional model stems from its origins in Gradient Boosting, leveraging ensemble-based trees to enhance processing speed and improve model robustness compared to basic methods. One of the key advantages of XGBoost is its ability to handle categorical variables directly, without requiring preprocessing. However, it is significantly more complex than previously applied models. In this case, we used the default hyperparameters.\n",
        "\n",
        "The model achieved a balanced accuracy of 70%, but it struggles to predict the minority class. For the minority class (1 - \"Employees who leave\"), only 41% are correctly identified, with an F1-score of 55%. This reflects moderate performance, as the model has a high number of false negatives.\n",
        "\n",
        "For Class 0 (\"Do not leave\"), all cases were correctly predicted. However, for Class 1 (\"Leave\"), there were 56 false negatives, meaning these employees were predicted as not leaving when they actually did. These results suggest that while XGBoost performs well overall, it requires further optimization or adjustments to improve its ability to capture the minority class. Either way, this model performed worse than our previously selected one, so without improvement, we would not consider it."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPBeW4tOvSZ25FoTvU2ULzo",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
